# Summer REU Project

(May 23, 2018)

Project Title (Tentative) : Toward developing a medical Natural Language Generation system using Deep Learning and Reinforcement Learning

Abstract
We aim to build a prototype framework for Natural Language Generation (NLG) for the medical field leveraging recent advances in Deep Learning and Reinforcement Learning. By defining the task of NLG as sequential decision-making process, we implement two baselines, one built upon Long Short-Term Memory (LSTM)-based Recurrent Neural Network (RNN) and the other with Monte Carlo Tree Search (MCTS). Later, other enhanced variants based on MCTS are further pursued while research focuses for roles of evaluation metric, target corpus, and algorithmic benefits, etc are actively undertaking.

References

LSTM RNN 
* http://karpathy.github.io/2015/05/21/rnn-effectiveness/
* Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.
* More to come

MCTS
* Kumagai, K., Kobayashi, I., Mochihashi, D., Asoh, H., Nakamura, T., & Nagai, T. (2016). Human-like Natural Language Generation Using Monte Carlo Tree Search. In Proceedings of the INLG 2016 Workshop on Computational Creativity in Natural Language Generation (pp. 11-18).
* More to come

Deep Reinforcement learning
A. Some sort of Introduction (Only for your information, Not for scientific works) 
* https://deepmind.com/ 
* (Just one hit randomly) https://www.wired.com/story/greedy-brittle-opaque-and-shallow-the-downsides-to-deep-learning/

Deep Learning

i. Artificial Neural Network models (since 2006, emphasized in its deep architecture)

  o End-to-end training with backpropagation
  
  o Scalable for large datasets
  
ii. Great performance for human-like perception problems (vision, NLP, audio, speech, etc). *biological sequences such as genomic and proteins. *some interests on graph datasets

iii. Machine Learning framework with representation learning, offering effective methods for unsupervised learning and other advanced methods such as meta learning, transfer learning, active learning, multi-task learning, multi modal learning, etc

iv. Deep Reinforcement Learning

  o Deep Learning topics with Reinforcement Learning techniques
  
  o Reinforcement Learning combined with Deep Learning-based function approximation
  
  * DNN, RNN, CNN, and my hybrid or complex architectures
  
  * RNN â€“ Long Short-Term Memory (LSTM)

Reinforcement Learning

i. MDP vs. POMDP

ii. Monte-Carlo Tree Search

Technical Requirements

Pre-requisite

A. PyTorch

B. Spacy

C. GitHub

D. Jupyter

E. Anadonda/PIP

F. Linux commands : ssh, etc

Note : There are numerous tutorials and youtube clips you can watch for more information. And, the most efficient way to master and to remember what you know is to repeat the same thing many times!

I. GitHub https://www.atlassian.com/git?utm_source=basic-git-commands&utm_medium=link&utm_campaign=git-microsite&_ga=2.75964245.1174987269.1527087707-1877757822.1527087707

Our repository location : http://github.com/

II. Python

a. Shell vs. Jupyter notebook

b. Package management with pip

https://pip.pypa.io/en/stable/

c. Environment with anaconda https://conda.io/docs/_downloads/conda-cheatsheet.pdf

III. Jupyter
